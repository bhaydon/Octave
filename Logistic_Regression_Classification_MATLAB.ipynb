{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNpVKdUmRMWKwTzKo16k89D",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhaydon/Octave/blob/main/Logistic_Regression_Classification_MATLAB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logistic Regression Classification using MATLAB/OCTAVE\n",
        "# Bruce Haydon"
      ],
      "metadata": {
        "id": "0Sidbb6Cp5zD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gz1buSYJp2_e"
      },
      "outputs": [],
      "source": [
        "\n",
        "function [all_theta] = oneVsAll(X, y, num_labels, lambda)\n",
        "%ONEVSALL trains multiple logistic regression classifiers and returns all\n",
        "%the classifiers in a matrix all_theta, where the i-th row of all_theta \n",
        "%corresponds to the classifier for label i\n",
        "%   [all_theta] = ONEVSALL(X, y, num_labels, lambda) trains num_labels\n",
        "%   logistic regression classifiers and returns each of these classifiers\n",
        "%   in a matrix all_theta, where the i-th row of all_theta corresponds \n",
        "%   to the classifier for label i\n",
        "%   <Bruce Haydon, (2022)>\n",
        "%\n",
        "% Code will return all classifier parameters in a matrix all_theta (K x (N+1))\n",
        "% Each row of all_theta correponsds to learned logistic regression parameters\n",
        "% for one class.\n",
        "%\n",
        "% \"y\" variable is vector of labels from 1-10 where \"0\" is mapped to label 10\n",
        "% y=m-dimensional vector of labels where y(j)=0|1, indicates whether jth \n",
        "%  training instance belongs to class k (y(j)=1) or if it belongs to a\n",
        "%  different class (y(j)=0).\n",
        "\n",
        "% Some useful variables\n",
        "m = size(X, 1)\n",
        "n = size(X, 2)\n",
        "\n",
        "% The following variables will be returned \n",
        "all_theta = zeros(num_labels, n + 1);\n",
        "\n",
        "% Add ones to the X data matrix\n",
        "X = [ones(m, 1) X];\n",
        "\n",
        "% ====================== YOUR CODE HERE ======================\n",
        "% Instructions: You should complete the following code to train num_labels\n",
        "%               logistic regression classifiers with regularization\n",
        "%               parameter lambda. \n",
        "%\n",
        "% Note: theta(:) will return a column vector.\n",
        "%\n",
        "%       **Logical Arrays in Octave/MATLAB**\n",
        "% Note: Use y == c to obtain a vector of 1's and 0's that tell you\n",
        "%       whether the ground truth is true/false for this class.\n",
        "%\n",
        "% Note: fmincg function is used to optimize the cost\n",
        "%       function. For-loop is used (for c = 1:num_labels) to\n",
        "%       loop over the different classes.\n",
        "%\n",
        "%       fmincg works similarly to fminunc, but is more efficient when we\n",
        "%       are dealing with large number of parameters.\n",
        "%\n",
        "% Sample Code for usage of fmincg:\n",
        "%\n",
        "%     % Set Initial theta\n",
        "%     initial_theta = zeros(n + 1, 1);\n",
        "%     \n",
        "%     % Set options for fminunc\n",
        "%     options = optimset('GradObj', 'on', 'MaxIter', 50);\n",
        "% \n",
        "%     % Run fmincg to obtain the optimal theta\n",
        "%     % This function will return theta and the cost \n",
        "%     [theta] = ...\n",
        "%         fmincg (@(t)(lrCostFunction(t, X, (y == c), lambda)), ...\n",
        "%                 initial_theta, options);\n",
        "\n",
        "\n",
        "%     % Set options for fminunc\n",
        "     options = optimset('GradObj', 'on', 'MaxIter', 50);\n",
        "     \n",
        "%     % Run fmincg to obtain the optimal theta\n",
        "%     % This function will return theta and the cost \n",
        "%      Need to loop through the K classes using count variable \"c\"   \n",
        "     \n",
        "     for c= 1:num_labels;\n",
        "%     % Set Initial theta to array of zeroes     \n",
        "      initial_theta = zeros(n+1,1);     \n",
        "      \n",
        "      [theta] = fmincg (@(t)(lrCostFunction(t, X, (y == c), lambda)), ...\n",
        "         initial_theta, options); \n",
        "         \n",
        "%      %assign calculated theta for this class to all_theta vector\n",
        "%      %in cth row    \n",
        "       all_theta(c,:) = theta';\n",
        "       \n",
        "    endfor;\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "% =========================================================================\n",
        "\n",
        "\n",
        "end"
      ]
    }
  ]
}